{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring and understanding Decision trees (for regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint, pformat\n",
    "from anytree import Node, RenderTree #To construct trees.\n",
    "import copy #Will use to deepcopy instances of the Node class.\n",
    "from sklearn.model_selection import KFold #For the K-fold cross validation.\n",
    "from simple_logger import *\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal:\n",
    "Make a regression decision tree constructor based on algorithm 8.1 of [Gareth], building from scratch all the functions present in this algorithm.\n",
    "Allow the functions involved to print the construction steps, so that one can visualize what is happening and gain a deeper understanding. It should be especially useful for someone learning Decision trees from [Gareth] and [Hastie].\n",
    "\n",
    "### Procedure:\n",
    "1. Construct a deep unpruned tree using recursive binary splitting.\n",
    "2. Define a function that prunes the deep unpruned tree using cost-complexity pruning. This process depends on a hyperparameter $\\alpha$.\n",
    "3. Select a good $\\alpha$ using $K$-fold cross-validation. Then 1+2 give us the decision tree.\n",
    "4. Apply to data also analyzed in [Gareth], to make sure our algorithm works.\n",
    "\n",
    "**Optional printing:*** To implement optional printing, I created a module called simple_logger which can be used to turn a normal function into a function with optional/debug printing, as long as one substitutes all `print()` calls by `log()` calls. You can read more about it and see a simple example in my GitHub repository about the simple_logger module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Binary Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_split(examples, predictor_to_split_idx, cutpoint):\n",
    "    '''\n",
    "    Performs a binary split on the examples array.\n",
    "    ------------\n",
    "    Parameters:\n",
    "    examples is a (# examples, p+1) numpy array where p is the number of predictors. The last column contains the responses.\n",
    "    Splits the predictor_to_split in two at cutpoint.\n",
    "    ------------\n",
    "    Returns a tuple with two (, p+1) numpy arrays of examples:\n",
    "        1: examples whose predictor_to_split is < cutpoint.\n",
    "        2: examples whose predictor_to_split is >= cutpoint.\n",
    "    '''\n",
    "    predictors_to_split = examples[:, predictor_to_split_idx] #Column vector with the value for the predictor to split for all the examples.\n",
    "    mask = predictors_to_split < cutpoint #Boolean mask with True values where predictor to split < cutpoint.\n",
    "    set1 = examples[mask, :]\n",
    "    set2 = examples[np.logical_not(mask), :] #Selects the examples where predictor to split >= cutpoint.\n",
    "    return set1, set2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xj values from in set1: [], cutpoint = 1, xj values from in set1: [2 5]\n",
      "xj values from in set1: [2], cutpoint = 3, xj values from in set1: [5]\n",
      "xj values from in set1: [2], cutpoint = 5, xj values from in set1: [5]\n"
     ]
    }
   ],
   "source": [
    "exs = np.array([[1,2,3],[1,5,6]])\n",
    "j = 1 #index of the split variable\n",
    "cutpoints = [1, 3, 5]\n",
    "for s in cutpoints:\n",
    "    set1, set2 = binary_split(exs, j, s)\n",
    "    xj_set1 = set1[:, j] #All the xjs in set1. These should all be < s.\n",
    "    xj_set2 = set2[:, j] #All the xjs in set2. These should all be >= s.\n",
    "    print('xj values from in set1: {}, cutpoint = {}, xj values from in set1: {}'.format(xj_set1, s, xj_set2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exs[0,:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's working!\n",
    "\n",
    "The following functions are a bit more complicated, so we allow for optional printing/logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function without logging.\n",
    "def no_log_optimal_binary_split(log, examples, predictors_to_split_indices, grid_step_number=10):\n",
    "    '''\n",
    "    Used to construct optimal_binary_split.\n",
    "    '''\n",
    "    p = len(examples[0, :-1]) #Number of predictors.\n",
    "    smallest_cost = 100000\n",
    "    best_split = (0, 0, smallest_cost) #placeholder for best_split.\n",
    "    for j in predictors_to_split_indices:\n",
    "        #Construct array of cutpoints:\n",
    "        max_cutpoint =  max(examples[:, j]) \n",
    "        step = (max_cutpoint - min(examples[:,j]))/grid_step_number #grid step\n",
    "        min_cutpoint =  min(examples[:, j]) + step  #We do not want to include min(examples[:,j]) itself, since that would lead to cases with no points 'on the left' because of how binary_split() was defined.\n",
    "        if min_cutpoint == max_cutpoint: #Examples have the same jth predictor value, so no split can be made, so we abort the split.\n",
    "            log('Skip j = {}'.format(j))\n",
    "            continue #Goes to the top of this loop again.\n",
    "        cutpoints = np.linspace(min_cutpoint, max_cutpoint, grid_step_number) \n",
    "        log('Cutpoints for j={}: {}, step: {}'.format(j, cutpoints, step))\n",
    "        for s in cutpoints:\n",
    "            set1, set2 = binary_split(examples, j, s)\n",
    "            log('s,j: {}, {} \\n set1 --- set2: {} --- {}'.format(s,j, set1, set2))\n",
    "            y_1 = set1[:, p]; y_2 = set2[:, p] #Extract the responses.\n",
    "            y1_estimate = np.average(y_1); y2_estimate = np.average(y_2) #Estimates will simply be the averages.\n",
    "            cost = np.sum(np.square(y_1 - y1_estimate)) + np.sum(np.square(y_2 - y2_estimate))\n",
    "            if cost < smallest_cost:\n",
    "                log('NEW COST: {}'.format(cost))\n",
    "                smallest_cost = cost\n",
    "                best_split = (j, s, smallest_cost) #Store info about this iteration.\n",
    "    return best_split\n",
    "\n",
    "# Add logging control using my simple_logger module.\n",
    "def optimal_binary_split(examples, predictors_to_split_indices, grid_step_number=10, debug_prints=False):\n",
    "    '''\n",
    "    Splits the examples array at the best cutpoint and using the best predictor.\n",
    "    ------------\n",
    "    Parameters:\n",
    "    examples is a (# examples, p+1) numpy array where p is the number of predictors. The last column contains the responses.\n",
    "    predictors_to_split_indices is a list containing the indices of the predictors we want to split. It can also be the string 'all', in which case we split all the predictors.\n",
    "    ------------\n",
    "    Returns:\n",
    "    Tuple (j, s, smallest_cost):\n",
    "        1) index of the predictor that was split.\n",
    "        2) chosen cutpoint for the split.\n",
    "        3) cost of the chosen (thus optimal) split.\n",
    "    '''\n",
    "    return with_logger(debug_prints, no_log_optimal_binary_split, examples, predictors_to_split_indices, grid_step_number=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skip j = 0\n",
      "Cutpoints for j=1: [2.3 2.6 2.9 3.2 3.5 3.8 4.1 4.4 4.7 5. ], step: 0.3\n",
      "s,j: 2.3, 1 \n",
      " set1 --- set2: [[1 2 8]] --- [[1 4 7]\n",
      " [1 5 2]]\n",
      "NEW COST: 12.5\n",
      "s,j: 2.5999999999999996, 1 \n",
      " set1 --- set2: [[1 2 8]] --- [[1 4 7]\n",
      " [1 5 2]]\n",
      "s,j: 2.9, 1 \n",
      " set1 --- set2: [[1 2 8]] --- [[1 4 7]\n",
      " [1 5 2]]\n",
      "s,j: 3.2, 1 \n",
      " set1 --- set2: [[1 2 8]] --- [[1 4 7]\n",
      " [1 5 2]]\n",
      "s,j: 3.5, 1 \n",
      " set1 --- set2: [[1 2 8]] --- [[1 4 7]\n",
      " [1 5 2]]\n",
      "s,j: 3.8, 1 \n",
      " set1 --- set2: [[1 2 8]] --- [[1 4 7]\n",
      " [1 5 2]]\n",
      "s,j: 4.1, 1 \n",
      " set1 --- set2: [[1 4 7]\n",
      " [1 2 8]] --- [[1 5 2]]\n",
      "NEW COST: 0.5\n",
      "s,j: 4.4, 1 \n",
      " set1 --- set2: [[1 4 7]\n",
      " [1 2 8]] --- [[1 5 2]]\n",
      "s,j: 4.7, 1 \n",
      " set1 --- set2: [[1 4 7]\n",
      " [1 2 8]] --- [[1 5 2]]\n",
      "s,j: 5.0, 1 \n",
      " set1 --- set2: [[1 4 7]\n",
      " [1 2 8]] --- [[1 5 2]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples: \n",
      " [[1 4 7]\n",
      " [1 5 2]\n",
      " [1 2 8]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 4.1, 0.5)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example.\n",
    "exs = np.array([[1,4,7],[1,5,2],[1,2,8]])\n",
    "print('Examples: \\n {}'.format(exs))\n",
    "p = len(exs[0, :-1]) #Number of predictors.\n",
    "predictors_to_split_indices = range(p) #List with indices of predictors to split.\n",
    "\n",
    "optimal_binary_split(exs, predictors_to_split_indices, debug_prints=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the function behaves as expected, clumping together the first and third rows, which indeed have the closest response values 7 and 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_log_recursive_binary_split(log, examples, max_leaf_population, predictors_to_split_indices, grid_step_number, deep_debug_prints):\n",
    "    '''\n",
    "    Used to construct recursive_binary_split.\n",
    "    '''\n",
    "    p = len(examples[0, :-1]) #Number of predictors.\n",
    "    if predictors_to_split_indices == 'all':\n",
    "        predictors_to_split_indices = range(p) #List with indices of predictors to split.\n",
    "    #Initialize variables.\n",
    "    tree_root = Node(examples, id='root'); tree_root.region = [] #Initialize tree with the root node.\n",
    "    top_leaf_population = len(examples[:,0])\n",
    "    while top_leaf_population >= max_leaf_population:\n",
    "        #Create list of region leaf nodes.\n",
    "        regions = tree_root.leaves\n",
    "        #Select best region to make split, and split parameters.\n",
    "        splits_info = [((idx, region), optimal_binary_split(region.name, predictors_to_split_indices, grid_step_number, debug_prints=deep_debug_prints)) for (idx,region) in enumerate(regions) if region.name.shape[0]>0] #The if statement ensures there's at least one example. region.name is the array of examples in the region node.\n",
    "        #Find best split.\n",
    "        costs = [info[1][2] for info in splits_info] #info[1] is tuple (j,s,smallest_cost)\n",
    "        min_idx = np.argmin(costs)\n",
    "        best_split = splits_info[min_idx] #Tuple ((idx, region), (j,s,cost))\n",
    "        #We now make the actual split.\n",
    "        idx_region, region = best_split[0]\n",
    "        j = best_split[1][0]\n",
    "        s = best_split[1][1]\n",
    "        R1, R2 = binary_split(region.name, j, s)\n",
    "        #Create new leaves, update tree and leaf pop:\n",
    "        leaf1 = Node(R1, id='x_{} < {}'.format(j,s)); leaf1.region = region.region + [(j, s, True)] #Add new region constraints to the parent's contraints.\n",
    "        leaf2 = Node(R2, id='x_{} >= {}'.format(j,s)); leaf2.region = region.region + [(j, s, False)]\n",
    "        region.children += (leaf1, leaf2,)\n",
    "        top_leaf_population = max([len(leaf.name[:,0]) for leaf in tree_root.leaves])\n",
    "        #Logs for debugging:\n",
    "        log('--------------------')\n",
    "        log('Two representations of the current tree:'); log('')\n",
    "        log(RenderTree(tree_root).by_attr('id'));  log('')\n",
    "        log(RenderTree(tree_root).by_attr('name'));  log('')\n",
    "        log('Current max leaf population: {}'.format(top_leaf_population))\n",
    "    return tree_root\n",
    "\n",
    "def recursive_binary_split(examples, max_leaf_population, predictors_to_split_indices='all', grid_step_number=10, debug_prints=False, deep_debug_prints=False):\n",
    "    '''\n",
    "    Recursively splits the examples array, building a decision tree with binary splits and stopping when every leaf contains less than max_leaf_population examples.\n",
    "    ------------\n",
    "    Parameters:\n",
    "    examples is a (# examples, p+1) numpy array where p is the number of predictors. The last column contains the responses.\n",
    "    predictors_to_split_indices is a list containing the indices of the predictors we want to split. It can also be the string 'all', in which case we split all the predictors.\n",
    "    ------------\n",
    "    Returns:\n",
    "    tree_root: the root node of the resulting tree.\n",
    "    '''\n",
    "    return with_logger(debug_prints, no_log_recursive_binary_split, examples, max_leaf_population, predictors_to_split_indices, grid_step_number, deep_debug_prints) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Two representations of the current tree:\n",
      "\n",
      "root\n",
      "├── x_1 < 4.1\n",
      "└── x_1 >= 4.1\n",
      "\n",
      "[[1 4 7]\n",
      " [1 5 2]\n",
      " [1 2 8]]\n",
      "├── [[1 4 7]\n",
      "│    [1 2 8]]\n",
      "└── [[1 5 2]]\n",
      "\n",
      "Current max leaf population: 2\n",
      "--------------------\n",
      "Two representations of the current tree:\n",
      "\n",
      "root\n",
      "├── x_1 < 4.1\n",
      "│   ├── x_1 < 2.2\n",
      "│   └── x_1 >= 2.2\n",
      "└── x_1 >= 4.1\n",
      "\n",
      "[[1 4 7]\n",
      " [1 5 2]\n",
      " [1 2 8]]\n",
      "├── [[1 4 7]\n",
      "│    [1 2 8]]\n",
      "│   ├── [[1 2 8]]\n",
      "│   └── [[1 4 7]]\n",
      "└── [[1 5 2]]\n",
      "\n",
      "Current max leaf population: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples: \n",
      " [[1 4 7]\n",
      " [1 5 2]\n",
      " [1 2 8]]\n",
      "\n",
      " The final tree: \n",
      " [[1 4 7]\n",
      " [1 5 2]\n",
      " [1 2 8]]\n",
      "├── [[1 4 7]\n",
      "│    [1 2 8]]\n",
      "│   ├── [[1 2 8]]\n",
      "│   └── [[1 4 7]]\n",
      "└── [[1 5 2]]\n",
      "\n",
      " The final tree (encoded) region of the first leaf: \n",
      " [(1, 4.1, True), (1, 2.2, True)]\n"
     ]
    }
   ],
   "source": [
    "#Example.\n",
    "exs = np.array([[1,4,7],[1,5,2],[1,2,8]])\n",
    "print('Examples: \\n {}'.format(exs))\n",
    "\n",
    "tree_root = recursive_binary_split(exs, 2, debug_prints=True, deep_debug_prints=False)\n",
    "print('\\n The final tree: \\n', RenderTree(tree_root).by_attr('name'))\n",
    "print('\\n The final tree (encoded) region of the first leaf: \\n', tree_root.leaves[0].region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree prunning [Hastie]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tree $T_0$ created using recursive binary split will probably overfit. The strategy is to simplify the model by finding an appropriate subtree $T$ of $T_0$ by pruning $T$ using *cost-complexity-pruning*, which we'll briefly explain here.\n",
    "\n",
    "The *cost complexity criterion* is defined by\n",
    "$$\n",
    "C_\\alpha(T) = \\sum_{m=1}^{\\vert T \\vert} \\sum_{i\\in I_m} (y_i - \\hat{y}_{R_m})^2 + \\alpha \\vert T \\vert\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\vert T \\vert$ is the number of leaves of $T$, $I_m$ is the indexing set of the region $R_m$ (*i.e.* $R_m = \\{x_i \\in \\text{examples}: i\\in I_m\\}$), $\\hat{y}_{R_m}$ is the predicted response in $R_m$ (in our case $\\hat{y}_{R_m}$ is just the mean $\\mu_{R_m}$), and $\\alpha\\in \\mathbb{R}^+$ controls the size of the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to find the subtree $T_\\alpha\\subseteq T_0$ that minimizes $C_\\alpha(T)$. Notice that for $\\alpha = 0$ the minimizing subtree is $T_0$ itself, thus justifying the notation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be shown [Breiman] that for all $\\alpha\\in\\mathbb{R}^+$ there is a unique smallest subtree $T_\\alpha$ that minimizes the cost complexity criterion, and to find it one can use *weakest link pruning*: starting from the bottom (the leaves) of the tree $T_0$, undo the split which has less impact (decreases the least) on the $\\sum_{m=1}^{\\vert T \\vert} \\sum_{i\\in I_m} (y_i - \\hat{y}_{R_m})^2$ part of the cost complexity criterion, obtaining a subtree; keep doing this until you're left only with the root of the tree.\n",
    "This gives us a sequence of subtrees of $T_0$, and it turns out [Breiman] that this sequence must contain $T_\\alpha$.\n",
    "\n",
    "This means that we can simply implement weakest link pruning to obtain a sequence of subtrees and find the one which minimizes $C_\\alpha$. That subtree must be $T_\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost-complexity criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_complexity_criterion(alpha, regions):\n",
    "    '''\n",
    "    Computes the cost complexity criterion using the average as the in-region prediction.\n",
    "    ------------\n",
    "    Parameters:\n",
    "    alpha >= 0.\n",
    "    regions is a list of 'regions', each 'region' being an array of examples.\n",
    "    ------------\n",
    "    Returns:\n",
    "    cost: real positive number.\n",
    "    '''\n",
    "    cost = 0\n",
    "    leaf_number = len(regions)\n",
    "    for region in regions:\n",
    "        ys = region[:,-1] #Extract responses.\n",
    "        pred = np.mean(ys) #Decision trees usually predict using the in-region mean.\n",
    "        sq_dev = np.sum(np.square( ys - pred ))\n",
    "        cost += sq_dev + alpha*leaf_number\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leaves:  [array([[1, 2, 8]]), array([[1, 4, 7]]), array([[1, 5, 2]])]\n",
      "alpha: 0, cost: 0.0\n",
      "alpha: 0.5, cost: 4.5\n",
      "alpha: 1, cost: 9.0\n",
      "alpha: 2, cost: 18.0\n",
      "alpha: 10, cost: 90.0\n",
      "leaves:  [array([[1, 4, 7],\n",
      "       [1, 2, 8]]), array([[1, 5, 2]])]\n",
      "alpha: 0, cost: 0.5\n",
      "alpha: 0.5, cost: 2.5\n",
      "alpha: 1, cost: 4.5\n",
      "alpha: 2, cost: 8.5\n",
      "alpha: 10, cost: 40.5\n"
     ]
    }
   ],
   "source": [
    "#Let's test this on the example from before:\n",
    "alphas = [0, 0.5, 1, 2, 10]\n",
    "example_leaves = recursive_binary_split(exs, 2).leaves\n",
    "leaves = [leaf.name for leaf in example_leaves]\n",
    "print('leaves: ', leaves)\n",
    "for alpha in alphas:\n",
    "    print('alpha: {}, cost: {}'.format(alpha, cost_complexity_criterion(alpha, leaves)) )\n",
    "\n",
    "# But what if we allow for 2 examples per region when splitting?:\n",
    "example_leaves = recursive_binary_split(exs, 3).leaves\n",
    "leaves = [leaf.name for leaf in example_leaves]\n",
    "print('leaves: ', leaves)\n",
    "for alpha in alphas:\n",
    "    print('alpha: {}, cost: {}'.format(alpha, cost_complexity_criterion(alpha, leaves)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the results make sense: at $\\alpha=0$ the cost is zero (so of course minimum) for the more complex tree (the first one). But for the other values of alpha one sees that the cost of the simpler tree (the second one) gives a lower cost! This is precisely the kind of behavior we wanted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weakest-link pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to define weakest link pruning, we will use the fact that each pair of siblings in the tree correspond exactly to a split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree: \n",
      " [[1 4 7]\n",
      " [1 5 2]\n",
      " [1 2 8]]\n",
      "├── [[1 4 7]\n",
      "│    [1 2 8]]\n",
      "│   ├── [[1 2 8]]\n",
      "│   └── [[1 4 7]]\n",
      "└── [[1 5 2]] \n",
      "\n",
      "Descendants of the root and their siblings:\n",
      "descendant:\n",
      "[[1 4 7]\n",
      " [1 2 8]]\n",
      "   sibling:[array([[1, 5, 2]])]\n",
      "descendant:\n",
      "[[1 2 8]]\n",
      "   sibling:[array([[1, 4, 7]])]\n",
      "descendant:\n",
      "[[1 4 7]]\n",
      "   sibling:[array([[1, 2, 8]])]\n",
      "descendant:\n",
      "[[1 5 2]]\n",
      "   sibling:[array([[1, 4, 7],\n",
      "       [1, 2, 8]])]\n"
     ]
    }
   ],
   "source": [
    "#Using again the above example:\n",
    "print('Tree: \\n', RenderTree(tree_root).by_attr('name'), '\\n\\nDescendants of the root and their siblings:')\n",
    "for descendant in tree_root.descendants:\n",
    "    print('descendant:\\n{}\\n   sibling:{}'.format(descendant.name, [sib.name for sib in descendant.siblings]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the algorithm of the weakest link pruning (acting on the tree $T$) will:\n",
    "   1. prune away one pair of sibling leaves, creating a temporary subtree $T'$ of $T$.\n",
    "   2. compute the complexity cost criterion of $T'$.\n",
    "   3. repeat 1,2 for all pairs of sibling leaves.\n",
    "   4. select the subtree $T'$ with the lowest cost, and do $T=T'$.\n",
    "   5. repeat 1-4 until $T= \\{\\mathrm{root}\\}$ -- or equivalently until the height of $T$ is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sibling_pairs(tree_root):\n",
    "    leaves = tree_root.leaves\n",
    "    sib_pairs = []\n",
    "    for leaf in tree_root.leaves:\n",
    "        if any(leaf in pair for pair in sib_pairs): #We don't want duplicates, so we move on to the next leaf if this one is already in a pair.\n",
    "            continue\n",
    "        sib_pairs.append((leaf, leaf.siblings[0])) #Each leaf will have a unique sibling.\n",
    "    return sib_pairs\n",
    "\n",
    "def prune_siblings(sibs):\n",
    "    '''\n",
    "    sibs is tuple.\n",
    "    '''\n",
    "    parent = sibs[0].parent\n",
    "    parent.children = [] #This erases the siblings.\n",
    "    return \n",
    "\n",
    "def wl_prun_step(tree_root, alpha): #Contains steps 1-4 of the algorithm.\n",
    "    '''\n",
    "    One step of the weakest_link_pruning algorithm.\n",
    "    ---------\n",
    "    Returns:\n",
    "        tree_root: the root node of the tree after the pruning step.\n",
    "    '''\n",
    "    cost = 100000000\n",
    "    sib_pairs = get_sibling_pairs(tree_root)\n",
    "    for i in range(len(sib_pairs)): #Will go through all leaf sibling pairs. Must do this this way to allow for deep copies on every iteration.\n",
    "        tree_temp = copy.deepcopy(tree_root) #Copy the tree for manipulation.\n",
    "        #Select and prune pair:\n",
    "        sib_pairs_temp = get_sibling_pairs(tree_temp)\n",
    "        pair = sib_pairs_temp[i]\n",
    "        prune_siblings(pair)\n",
    "        #Compute cost of T':\n",
    "        leaves_nodes = tree_temp.leaves\n",
    "        leaves = [leaf.name for leaf in example_leaves] #We must feed arrays to the cost function, not nodes.\n",
    "        cost_temp = cost_complexity_criterion(alpha, leaves)\n",
    "        if cost_temp < cost:\n",
    "            cost = cost_temp\n",
    "            best_tree_temp = copy.deepcopy(tree_temp)\n",
    "    tree_root = copy.deepcopy(best_tree_temp) #Step 4 of the algorithm.\n",
    "    return tree_root\n",
    "    \n",
    "def no_log_weakest_link_pruning(log, tree_root, alpha):\n",
    "    '''\n",
    "    Used to define weakest_ling_pruning\n",
    "    '''\n",
    "    pruning_log = [( copy.deepcopy(tree_root), cost_complexity_criterion(alpha, [leaf.name for leaf in tree_root.leaves]) )]\n",
    "    itr = 0\n",
    "    log('iter: {}\\n{}\\n----------'.format(itr, RenderTree(tree_root).by_attr('id')))\n",
    "    while tree_root.height > 0:\n",
    "        itr += 1\n",
    "        tree_root = wl_prun_step(tree_root, alpha)\n",
    "        log('iter: {}\\n{}\\n----------'.format(itr, RenderTree(tree_root).by_attr('id')))\n",
    "        pruning_log.append(( copy.deepcopy(tree_root), cost_complexity_criterion(alpha, [leaf.name for leaf in tree_root.leaves]) ))\n",
    "    return pruning_log\n",
    "\n",
    "def weakest_link_pruning(tree_root, alpha, debug_prints=True):\n",
    "    '''\n",
    "    Implements weakest link pruning using the cost-complexity criterion, giving a sequence of pruned trees.\n",
    "    ---------------\n",
    "    Returns:\n",
    "        pruning_log: list of all the trees (or actually their root Nodes) resulting from the pruning.\n",
    "    '''\n",
    "    return with_logger(debug_prints, no_log_weakest_link_pruning, tree_root, alpha) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      "├── x_1 < 4.1\n",
      "│   ├── x_1 < 2.2\n",
      "│   └── x_1 >= 2.2\n",
      "└── x_1 >= 4.1\n",
      "\n",
      " Pairs:\n",
      " [(Node('/[[1 4 7]\\n [1 5 2]\\n [1 2 8]]/[[1 4 7]\\n [1 2 8]]/[[1 2 8]]', id='x_1 < 2.2', region=[(1, 4.1, True), (1, 2.2, True)]), Node('/[[1 4 7]\\n [1 5 2]\\n [1 2 8]]/[[1 4 7]\\n [1 2 8]]/[[1 4 7]]', id='x_1 >= 2.2', region=[(1, 4.1, True), (1, 2.2, False)])), (Node('/[[1 4 7]\\n [1 5 2]\\n [1 2 8]]/[[1 5 2]]', id='x_1 >= 4.1', region=[(1, 4.1, False)]), Node('/[[1 4 7]\\n [1 5 2]\\n [1 2 8]]/[[1 4 7]\\n [1 2 8]]', id='x_1 < 4.1', region=[(1, 4.1, True)]))]\n",
      "\n",
      "Deleting the first pair: \n",
      " root\n",
      "├── x_1 < 4.1\n",
      "└── x_1 >= 4.1\n"
     ]
    }
   ],
   "source": [
    "#Test the first and second functions with the example from before:\n",
    "tree_root = recursive_binary_split(exs, 2)\n",
    "tree_og = copy.deepcopy(tree_root)\n",
    "print(RenderTree(tree_root).by_attr('id'))\n",
    "print('\\n Pairs:\\n', get_sibling_pairs(tree_root))\n",
    "pair = get_sibling_pairs(tree_root)[0]\n",
    "prune_siblings(pair)\n",
    "print('\\nDeleting the first pair: \\n', RenderTree(tree_root).by_attr('id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iter: 0\n",
      "root\n",
      "├── x_1 < 4.1\n",
      "│   ├── x_1 < 2.2\n",
      "│   └── x_1 >= 2.2\n",
      "└── x_1 >= 4.1\n",
      "----------\n",
      "iter: 1\n",
      "root\n",
      "├── x_1 < 4.1\n",
      "└── x_1 >= 4.1\n",
      "----------\n",
      "iter: 2\n",
      "root\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- Log ----------\n",
      "iter: 0, cost: 7.200000000000001\n",
      "root\n",
      "├── x_1 < 4.1\n",
      "│   ├── x_1 < 2.2\n",
      "│   └── x_1 >= 2.2\n",
      "└── x_1 >= 4.1\n",
      "\n",
      "iter: 1, cost: 3.7\n",
      "root\n",
      "├── x_1 < 4.1\n",
      "└── x_1 >= 4.1\n",
      "\n",
      "iter: 2, cost: 21.466666666666665\n",
      "root\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Test the pruning:\n",
    "tree_root = recursive_binary_split(exs, 2)\n",
    "\n",
    "alpha = 0.8\n",
    "pruning_log = weakest_link_pruning(tree_root, alpha)\n",
    "\n",
    "#See if the log has all the subtress that we expect:\n",
    "print('----------- Log ----------')\n",
    "for idx,(tree,cost) in enumerate(pruning_log):\n",
    "    print('iter: {}, cost: {}\\n{}\\n'.format(idx, cost, RenderTree(tree).by_attr('id')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost-complexity pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we want to select, from the sequence, the tree with the smallest cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_complexity_pruning(tree_root, alpha):\n",
    "    '''\n",
    "    Returns: tuple (best_tree, min_cost)\n",
    "    '''\n",
    "    pruning_log = weakest_link_pruning(tree_root, alpha) #The entries are of type (tree_root, cost)\n",
    "    best_tree, min_cost = min(pruning_log, key=lambda p: p[1]) #  p[1]=cost.\n",
    "    return best_tree, min_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iter: 0\n",
      "root\n",
      "├── x_1 < 4.1\n",
      "│   ├── x_1 < 2.2\n",
      "│   └── x_1 >= 2.2\n",
      "└── x_1 >= 4.1\n",
      "----------\n",
      "iter: 1\n",
      "root\n",
      "├── x_1 < 4.1\n",
      "└── x_1 >= 4.1\n",
      "----------\n",
      "iter: 2\n",
      "root\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost:  3.7 \n",
      "Tree Talpha with alpha=0.8:\n",
      " root\n",
      "├── x_1 < 4.1\n",
      "└── x_1 >= 4.1\n"
     ]
    }
   ],
   "source": [
    "#Back to the same example as before:\n",
    "tree_root = recursive_binary_split(exs, 2)\n",
    "\n",
    "alpha = 0.8\n",
    "best_tree, min_cost = cost_complexity_pruning(tree_root, alpha)\n",
    "print('cost: ', min_cost, '\\nTree Talpha with alpha=0.8:\\n', RenderTree(best_tree).by_attr('id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the costs of each tree in the above cells (3.7 for this tree, larger than that for the others), we see that we obtained the correct $T_\\alpha$ from the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting $\\alpha$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to select a good value for alpha we use $K$-fold cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More exactly, given a set of examples and a list of values for $\\alpha$ we need to:\n",
    "1. Divide it in $K$ equal parts/folds.\n",
    "2. Construct $K$ different pairs (training set, test set) using those parts.\n",
    "3. For each of those pairs:\n",
    "    1. Construct the unprunned tree $T^K$.\n",
    "    2. For each $\\alpha$:\n",
    "        1. Construct the optimal tree $T_{\\alpha}$.\n",
    "        2. Compute the MSE (denoted $\\mathrm{MSE}^K_{\\alpha}$) on the test set (the $K$-fold we left out of the training set).\n",
    "4. Compute the cross-validation $\\mathrm{MSE}_\\alpha$ for the all the values of $\\alpha$, which is the average over K of the $\\mathrm{MSE}^K_{\\alpha}$.\n",
    "5. Select the $\\alpha$ whose $\\mathrm{MSE}_\\alpha$ is smallest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2] [0]\n",
      "[0 2] [1]\n",
      "[0 1] [2]\n"
     ]
    }
   ],
   "source": [
    "# 3-folding in our example:\n",
    "kf = KFold(n_splits=3)\n",
    "for train_idx, test_idx in kf.split(exs):\n",
    "    print(train_idx, test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_region(tree_root, x):\n",
    "    '''\n",
    "    x is a one dimensional array.\n",
    "    --------\n",
    "    Returns: leaf (node) corresponding to the region where x is.\n",
    "    '''\n",
    "    leaves = tree_root.leaves\n",
    "    for leaf in leaves:\n",
    "        encoded_region = leaf.region  # [(j,s,smaller?),...,(j,s,smaller?)]\n",
    "        #for j,s,smaller in encoded_region:\n",
    "         #   if (x[j] < s) == smaller: # True if xj<s and smaller=True or if xj>=s and smaller=False. False otherwise.\n",
    "        bools = [ (x[j] < s) == smaller for j,s,smaller in encoded_region ]\n",
    "        if all(bools):\n",
    "            return leaf\n",
    "    return 'x not in the domain?!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_1 >= 4.1\n",
      "x_1 >= 4.1\n",
      "x_1 < 4.1\n"
     ]
    }
   ],
   "source": [
    "#Test find_region\n",
    "print(find_region(best_tree, [1, 4.2]).id)\n",
    "print(find_region(best_tree, [1, 4.1]).id)\n",
    "print(find_region(best_tree, [1, 4.0]).id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions_from_tree(tree_root, xs):\n",
    "    '''\n",
    "    Returns list of predictions, one for each x in xs.\n",
    "    '''\n",
    "    preds = []\n",
    "    for x in xs:\n",
    "        #Get the region:\n",
    "        region_leaf = find_region(tree_root, x)\n",
    "        region_examples = region_leaf.name #Extract matrix of examples.\n",
    "        pred = np.mean(region_examples[:,-1]) #Decision trees predict by simply averaging the responses of the examples.\n",
    "        preds.append(pred)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      "├── x_1 < 4.1\n",
      "└── x_1 >= 4.1 \n",
      "\n",
      "[[1 4 7]\n",
      " [1 5 2]\n",
      " [1 2 8]]\n",
      "├── [[1 4 7]\n",
      "│    [1 2 8]]\n",
      "└── [[1 5 2]] \n",
      "\n",
      "[2.0, 2.0, 7.5]\n"
     ]
    }
   ],
   "source": [
    "#test predpredictions_from_tree\n",
    "print(RenderTree(best_tree).by_attr('id'), '\\n')\n",
    "print(RenderTree(best_tree).by_attr('name'), '\\n')\n",
    "print(predictions_from_tree(best_tree, [ [1, 4.2], [6, 4.1], [2, 4.0] ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(preds, ys):\n",
    "    error = (np.square(preds - ys)).mean() \n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_log_choose_alpha(log, examples, alphas, K, max_leaf_population=3):\n",
    "    '''\n",
    "    Used to define choose_alpha.\n",
    "    '''\n",
    "    kf = KFold(n_splits=3, random_state=0) #We use the sklearn KFold class for the splitting.\n",
    "    mse_array = np.zeros((K, len(alphas)))#Initialize error array.\n",
    "    k = 0\n",
    "    for train_idx, test_idx in kf.split(exs): #Will have K iterations.\n",
    "        #Construct the training-test pairs:\n",
    "        train, test = examples[train_idx], examples[test_idx] \n",
    "        #Construct the unprunned tree:\n",
    "        tree_root = recursive_binary_split(train, max_leaf_population) \n",
    "        for alpha in alphas:\n",
    "            #Construct the optimal tree Talpha:\n",
    "            optimal_tree,_ = cost_complexity_pruning(tree_root, alpha)\n",
    "            log('fold: {}, alpha: {}, tree: {}'.format(k, alpha, RenderTree(optimal_tree).by_attr('id')))\n",
    "            #Compute the MSE^K_alpha:\n",
    "            xs = test[:,:-1]; ys = test[:,-1]\n",
    "            preds = predictions_from_tree(optimal_tree, xs)\n",
    "            mse_K_alpha = mse(preds, ys)\n",
    "            #Store the mse:\n",
    "            mse_array[k, alphas.index(alpha)] = mse_K_alpha\n",
    "        k += 1\n",
    "    #Compute the CV MSE for every alpha (over K, so over the axis 0):\n",
    "    MSE_CV = mse_array.mean(axis=0) #This is an array of shape (1, len(alphas))\n",
    "    log('MSE_CV: {}'.format(MSE_CV))\n",
    "    log('MSE_CV shape: {}'.format(MSE_CV.shape))\n",
    "    #Select the alpha which minimizes MSE_CV_alpha:\n",
    "    best_alpha = min(alphas, key=lambda alpha: MSE_CV[alphas.index(alpha)])\n",
    "    return best_alpha\n",
    "\n",
    "def choose_alpha(examples, alphas, K, max_leaf_population=3, debug_prints=False):\n",
    "    '''\n",
    "    alphas: list of values to test for alpha.\n",
    "    '''\n",
    "    return with_logger(debug_prints, no_log_choose_alpha, examples, alphas, K, max_leaf_population=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save data from a github repo to a file:\n",
    "#hitters_data = pd.read_csv('https://raw.githubusercontent.com/jcrouser/islr-python/master/data/Hitters.csv')\n",
    "#hitters_data.to_csv('Hitters_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "hitters_data = pd.read_csv('Hitters_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Player</th>\n",
       "      <th>AtBat</th>\n",
       "      <th>Hits</th>\n",
       "      <th>HmRun</th>\n",
       "      <th>Runs</th>\n",
       "      <th>RBI</th>\n",
       "      <th>Walks</th>\n",
       "      <th>Years</th>\n",
       "      <th>CAtBat</th>\n",
       "      <th>CHits</th>\n",
       "      <th>...</th>\n",
       "      <th>CRuns</th>\n",
       "      <th>CRBI</th>\n",
       "      <th>CWalks</th>\n",
       "      <th>League</th>\n",
       "      <th>Division</th>\n",
       "      <th>PutOuts</th>\n",
       "      <th>Assists</th>\n",
       "      <th>Errors</th>\n",
       "      <th>Salary</th>\n",
       "      <th>NewLeague</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-Andy Allanson</td>\n",
       "      <td>293</td>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>29</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>293</td>\n",
       "      <td>66</td>\n",
       "      <td>...</td>\n",
       "      <td>30</td>\n",
       "      <td>29</td>\n",
       "      <td>14</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>446</td>\n",
       "      <td>33</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-Alan Ashby</td>\n",
       "      <td>315</td>\n",
       "      <td>81</td>\n",
       "      <td>7</td>\n",
       "      <td>24</td>\n",
       "      <td>38</td>\n",
       "      <td>39</td>\n",
       "      <td>14</td>\n",
       "      <td>3449</td>\n",
       "      <td>835</td>\n",
       "      <td>...</td>\n",
       "      <td>321</td>\n",
       "      <td>414</td>\n",
       "      <td>375</td>\n",
       "      <td>N</td>\n",
       "      <td>W</td>\n",
       "      <td>632</td>\n",
       "      <td>43</td>\n",
       "      <td>10</td>\n",
       "      <td>475.0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-Alvin Davis</td>\n",
       "      <td>479</td>\n",
       "      <td>130</td>\n",
       "      <td>18</td>\n",
       "      <td>66</td>\n",
       "      <td>72</td>\n",
       "      <td>76</td>\n",
       "      <td>3</td>\n",
       "      <td>1624</td>\n",
       "      <td>457</td>\n",
       "      <td>...</td>\n",
       "      <td>224</td>\n",
       "      <td>266</td>\n",
       "      <td>263</td>\n",
       "      <td>A</td>\n",
       "      <td>W</td>\n",
       "      <td>880</td>\n",
       "      <td>82</td>\n",
       "      <td>14</td>\n",
       "      <td>480.0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-Andre Dawson</td>\n",
       "      <td>496</td>\n",
       "      <td>141</td>\n",
       "      <td>20</td>\n",
       "      <td>65</td>\n",
       "      <td>78</td>\n",
       "      <td>37</td>\n",
       "      <td>11</td>\n",
       "      <td>5628</td>\n",
       "      <td>1575</td>\n",
       "      <td>...</td>\n",
       "      <td>828</td>\n",
       "      <td>838</td>\n",
       "      <td>354</td>\n",
       "      <td>N</td>\n",
       "      <td>E</td>\n",
       "      <td>200</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>500.0</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-Andres Galarraga</td>\n",
       "      <td>321</td>\n",
       "      <td>87</td>\n",
       "      <td>10</td>\n",
       "      <td>39</td>\n",
       "      <td>42</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>396</td>\n",
       "      <td>101</td>\n",
       "      <td>...</td>\n",
       "      <td>48</td>\n",
       "      <td>46</td>\n",
       "      <td>33</td>\n",
       "      <td>N</td>\n",
       "      <td>E</td>\n",
       "      <td>805</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>91.5</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Player  AtBat  Hits  HmRun  Runs  RBI  Walks  Years  CAtBat  \\\n",
       "0     -Andy Allanson    293    66      1    30   29     14      1     293   \n",
       "1        -Alan Ashby    315    81      7    24   38     39     14    3449   \n",
       "2       -Alvin Davis    479   130     18    66   72     76      3    1624   \n",
       "3      -Andre Dawson    496   141     20    65   78     37     11    5628   \n",
       "4  -Andres Galarraga    321    87     10    39   42     30      2     396   \n",
       "\n",
       "   CHits  ...  CRuns  CRBI  CWalks  League Division PutOuts  Assists  Errors  \\\n",
       "0     66  ...     30    29      14       A        E     446       33      20   \n",
       "1    835  ...    321   414     375       N        W     632       43      10   \n",
       "2    457  ...    224   266     263       A        W     880       82      14   \n",
       "3   1575  ...    828   838     354       N        E     200       11       3   \n",
       "4    101  ...     48    46      33       N        E     805       40       4   \n",
       "\n",
       "   Salary  NewLeague  \n",
       "0     NaN          A  \n",
       "1   475.0          N  \n",
       "2   480.0          A  \n",
       "3   500.0          N  \n",
       "4    91.5          N  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hitters_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "\n",
    "1. [Hastie]  \n",
    "2. [Norvig]\n",
    "3. [Gareth]\n",
    "4. [Breiman]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
